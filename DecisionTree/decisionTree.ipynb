{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import  SparkContext\n",
    "sc = SparkContext('local', 'App')\n",
    "\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1  准备数据\n",
    "StumbleUpon Evergreen数据，来源于Kaggle中的一个题目StumbleUpon Evergreen Classification Challenge。\n",
    "\n",
    "StumbleUpon 是一个个性化推荐引擎，根据用户的兴趣行为给用户推荐网页，而有些网页内容是即时性（ephemeral）的，比如新闻股票网页（用户短暂感兴趣），有些网页是长久性的（evergreen）如体育，理财等（用户持续感兴趣）。现要分辨网页是ephemeral的还是evergreen的，以便向用户推荐更加准确的网页。\n",
    "\n",
    "这是一个二分类问题。\n",
    "\n",
    "查看StumbleUpon数据的详细信息：https:www.kaggle.com/c/stumbleupon/data\n",
    "\n",
    "下载StumbleUpon数据，train.tsv和test.tsv。\n",
    "\n",
    "分析train.tsv的字段：\n",
    "\n",
    "- 0~2列忽略，url，urlid对网站是否evergreen关系不大\n",
    "- 第3列，categorical features分类特征，网页分类如：business，health, sports……\n",
    "- 第4~25列，numerical features数值特征，有关此网页的数值特征，例如链接的数目，图片的比例等\n",
    "- 第26列，label分类标签，1表示evergreen（用户长久感兴趣的网页），0表示non-evergreen（用户短暂感兴趣的网页）\n",
    "\n",
    "test.tsv没有第26列的label分类标签，其余和test.tsv相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始导入数据...\n"
     ]
    }
   ],
   "source": [
    "## 读取train.tsv\n",
    "print(\"开始导入数据...\")\n",
    "rawDataWithHeader = sc.textFile(\"/usr/hduser/data/decisiontree/train.tsv\")\n",
    "## 取出前2项数据\n",
    "# rawDataWithHeader.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从运行结果看出：第一项是字段名(特征名称)不是数据（要进行删除）；每一项数据以制表符”\\t”分隔；每个字段前后都有双引号“””分隔。除此之外，在train.tsv表中有些字段有缺失值用”?”表示。\n",
    "\n",
    "针对以上问题对train.tsv进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始导入数据...\n",
      "共有：7395项数据\n"
     ]
    }
   ],
   "source": [
    "## 读取train.tsv\n",
    "print(\"开始导入数据...\")\n",
    "rawDataWithHeader = sc.textFile(\"/usr/hduser/data/decisiontree/train.tsv\")\n",
    "## 取第一项数据\n",
    "header = rawDataWithHeader.first()\n",
    "## 剔除字段名（特征名）行，取数据行\n",
    "rawData = rawDataWithHeader.filter(lambda x:x!=header)\n",
    "## 将双引号\"替换为空字符（剔除双引号）\n",
    "rData = rawData.map(lambda x:x.replace(\"\\\"\",\"\"))\n",
    "## 以制表符分割每一行\n",
    "lines = rData.map(lambda x: x.split(\"\\t\"))\n",
    "print(\"共有：\"+str(lines.count())+\"项数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['business',\n",
       " '0.789131',\n",
       " '2.055555556',\n",
       " '0.676470588',\n",
       " '0.205882353',\n",
       " '0.047058824',\n",
       " '0.023529412',\n",
       " '0.443783175',\n",
       " '0',\n",
       " '0',\n",
       " '0.09077381',\n",
       " '0',\n",
       " '0.245831182',\n",
       " '0.003883495',\n",
       " '1',\n",
       " '1',\n",
       " '24',\n",
       " '0',\n",
       " '5424',\n",
       " '170',\n",
       " '8',\n",
       " '0.152941176',\n",
       " '0.079129575',\n",
       " '0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0~2列忽略，url，urlid对网站是否evergreen关系不大\n",
    "lines.first()[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2  处理特征\n",
    "\n",
    "该数据集的第3个字段是alchemy_category网页分类，是一个离散值特征，要采用OneHotEncode的方式进行编码转换为数值特征，主要过程如下：\n",
    "\n",
    "1. 创建categoriesMap字典，key为网页类别名，value为数字（网页类别名的索引值），每个类别名对应一个索引值\n",
    "2. 根据categoriesMap字典查询每个alchemy_category特征值对应的索引值，例如business的索引值categoryIdx为2\n",
    "3. 根据categoryIdx=2，以OneHotEncodeer的方式转换为一个列表categoryFeatures List，该列表长度为14（统计所有网页类别），categoryIdx=2对应的列表为[0,0,1,0,0,0,0,0,0,0,0,0,0,0]。\n",
    "\n",
    "建立categoriesMap网页分类字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoriesMap = lines.map(lambda x: x[3]).distinct().zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，lines.map()表示处理之前读取的数据的每一行，.map(lambda fields: fileds[3])表示读取第3个字段，.distinct()保留不重复数据，.zipWithIndex()将第3个字段中不重复的数据进行编号，.collectAsMap()转换为dict字典格式\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business': 0,\n",
       " 'recreation': 1,\n",
       " 'health': 2,\n",
       " 'sports': 3,\n",
       " '?': 4,\n",
       " 'arts_entertainment': 5,\n",
       " 'science_technology': 6,\n",
       " 'gaming': 7,\n",
       " 'culture_politics': 8,\n",
       " 'computer_internet': 9,\n",
       " 'law_crime': 10,\n",
       " 'religion': 11,\n",
       " 'weather': 12,\n",
       " 'unknown': 13}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoriesMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将每个alchemy_category网页分类特征值转化为列表categoryFeatures List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "categoryIndex = categoriesMap[lines.first()[3]]\n",
    "OneHot = np.zeros(len(categoriesMap))\n",
    "OneHot[categoryIndex] = 1\n",
    "print(OneHot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于第4~25字段的数值特征，要转换为数值，用float函数将字符串转换为数值，同时简单处理缺失值”?”为0.\n",
    "整个处理特征的过程可以封装成一个函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert(v):\n",
    "    \"\"\"处理数值特征的转换函数\"\"\"\n",
    "    return (0 if v==\"?\" else float(v))\n",
    "\n",
    "def process_features(line, categoriesMap, featureEnd):\n",
    "    \"\"\"处理特征，line为字段行，categoriesMap为网页分类字典，featureEnd为特征结束位置，此例为25\"\"\"\n",
    "    ## 处理alchemy_category网页分类特征\n",
    "    categoryIdx = categoriesMap[line[3]]\n",
    "    OneHot = np.zeros(len(categoriesMap))\n",
    "    OneHot[categoryIdx] = 1\n",
    "    ## 处理数值特征\n",
    "    numericalFeatures = [convert(value) for value in line[4:featureEnd]]\n",
    "    # 返回拼接的总特征列表\n",
    "    return np.concatenate((OneHot, numericalFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、处理label分类标签\n",
    "\n",
    "定义一个函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_label(line):\n",
    "    return float(line[-1])  # 最后一个字段为类别标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、构建模型所需数据格式\n",
    "\n",
    "Spark Mllib分类任务支持的数据类型为LabeledPoint格式，LabeledPoint数据由标签label和特征feature组成。构建LabeledPoint数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "labelpointRDD = lines.map(lambda r: LabeledPoint(process_label(r), process_features(r,categoriesMap, len(r)-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelpointRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、划分训练集、验证集以及测试集\n",
    "\n",
    "按照7:1:2的比例划分训练集、验证集以及测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本个数：5159验证集样本个数：727测试集样本个数：1509\n"
     ]
    }
   ],
   "source": [
    "## 划分训练集、验证集和测试集\n",
    "(trainData, validationData, testData) = labelpointRDD.randomSplit([7,1,2])\n",
    "print(\"训练集样本个数：\"+str(trainData.count()) \n",
    "      + \"验证集样本个数：\"+str(validationData.count())\n",
    "      + \"测试集样本个数：\"+str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[20] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将数据暂存到内存中，加快后续运算效率\n",
    "trainData.persist()\n",
    "validationData.persist()\n",
    "testData.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 训练模型\n",
    "选择Spark MLlib中的决策树DecisionTree模块中的trainClassifier方法进行训练并建立模型：\n",
    "\n",
    "DecisionTree.trainClassifier(input, numClasses, categoricalFeaturesInfo, impurity,maxDepth,maxBins)\n",
    "参数说明如下：\n",
    "\n",
    "- (1) input：输入的训练数据，数据格式为LabeledPoint数据\n",
    "- (2) numClasses：指定分类数目\n",
    "- (3) categoricalFeaturesInfo：设置分类特征字段信息，本例采用OneHot编码处理分类特征字段，故这里设置为空字典dict()\n",
    "- (4) impurity：决策树的impurity评估方法（划分的度量选择）：gini基尼系数，entropy熵\n",
    "- (5) maxDepth：决策树最大深度\n",
    "- (6) maxBins：决策树每个节点的最大分支数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "model = DecisionTree.trainClassifier(trainData, numClasses=2, \n",
    "#                                      categoricalFeaturesInfo={}, impurity=\"entropy\", \n",
    "                                     categoricalFeaturesInfo={}, impurity=\"gini\",\n",
    "#                                      maxDepth=5, maxBins=5)\n",
    "                                     maxDepth=5, maxBins=32)\n",
    "\n",
    "# LogisticRegressionWithSGD.train(trainData, iterations=100, step=1.0, miniBatchFraction=1.0,\n",
    "#               initialWeights=None, regParam=0.01, regType=\"l2\", intercept=False,\n",
    "#               validateData=True, convergenceTol=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7   评估模型\n",
    "使用AUC(Area under the Curve of ROC)来对模型进行评估，接收者操作特征(Receiver Operating Characteristic , ROC)曲线是一种比较分类器模型有用的可视化工具。\n",
    "\n",
    "ROC曲线显示了给定模型的真正例率(TPR=TP/P)(纵轴)和假正例率(FPR=FP/N)(横轴)之间的权衡。TPR的增加以FPR的增加为代价。ROC曲线下方的面积是模型准确率的度量：AUC\n",
    "\n",
    "- AUC=1：预测准确率100%\n",
    "- 0.5 < AUC <1：优于随机猜测，具有预测意义\n",
    "- AUC=0.5: 与随机猜测一样，没有预测意义\n",
    "- AUC<0.5: 比随机预测还差\n",
    "\n",
    "Spark Mllib提供了BinaryClassificationMetrics计算AUC的方法。\n",
    "\n",
    "首先创建predict_real列表，列表的每个元素为一个元组(predict,real)，其中predict为预测结果，real为实际标签\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0), (0.0, 1.0), (0.0, 0.0), (0.0, 1.0), (0.0, 1.0)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 创建predict_real列表\n",
    "predict = model.predict(validationData.map(lambda p:p.features))\n",
    "predict_real = predict.zip(validationData.map(lambda p: p.label))\n",
    "predict_real.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着使用BinaryClassificationMetrics计算AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC=0.6575233325745504\n"
     ]
    }
   ],
   "source": [
    "## 使用BinaryClassificationMetrics计算AUC\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "metrics = BinaryClassificationMetrics(predict_real)\n",
    "print(\"AUC=\" + str(metrics.areaUnderROC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8  模型参数选择\n",
    "DecisionTree的参数impurity, maxDepth, maxBins会影响模型的准确率及训练的时间，下面对不同模型参数取值进行测试评估。\n",
    "\n",
    "创建trainEvaluateModel函数包含训练与评估功能，并计算训练评估的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 创建trainEvaluateModel函数包含训练与评估功能，并计算训练评估的时间。\n",
    "from time import time\n",
    "\n",
    "def trainEvaluateModel(trainData, validationData, impurityParm, maxDepthParm, maxBinsParm):\n",
    "    startTime = time()\n",
    "    ## 创建并训练模型\n",
    "    model = DecisionTree.trainClassifier(trainData, numClasses=2,categoricalFeaturesInfo={}, \n",
    "                                         impurity=impurityParm, maxDepth=maxDepthParm,maxBins=maxBinsParm)\n",
    "    ## 计算AUC\n",
    "    predict = model.predict(validationData.map(lambda p:p.features))\n",
    "    predict_real = predict.zip(validationData.map(lambda p: p.label))\n",
    "    metrics = BinaryClassificationMetrics(predict_real)\n",
    "    AUC = metrics.areaUnderROC\n",
    "    duration = time() - startTime   # 持续时间\n",
    "#     print(\"训练评估：参数\"+\"impurity=\"+str(impurityParm) + \n",
    "#          \"   maxDepth=\"+str(maxDepthParm)+\"   maxBins=\"+str(maxBinsParm)+\"\\n\"+\n",
    "#          \"===>消耗时间=\"+str(duration)+\"   结果AUC=\"+str(AUC))\n",
    "    return AUC, duration, impurityParm, maxDepthParm, maxBinsParm, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、评估impurity参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 评估impurity参数\n",
    "impurityList=[\"gini\",\"entropy\"]\n",
    "maxDepthList = [10]\n",
    "maxBinsList = [10]\n",
    "\n",
    "## 返回结果存放至metries中\n",
    "metrics = [trainEvaluateModel(trainData, validationData, impurity, maxDepth, maxBins)\n",
    "          for impurity in impurityList\n",
    "          for maxDepth in maxDepthList\n",
    "          for maxBins in maxBinsList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.6587335913195235, 1.0970885753631592, 'gini', 10, 10, DecisionTreeModel classifier of depth 10 with 527 nodes), (0.633397829880871, 0.7491676807403564, 'entropy', 10, 10, DecisionTreeModel classifier of depth 10 with 535 nodes)]\n"
     ]
    }
   ],
   "source": [
    "# 查看metrics\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>duration</th>\n",
       "      <th>impurity</th>\n",
       "      <th>maxDepth</th>\n",
       "      <th>maxBins</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gini</th>\n",
       "      <td>0.658734</td>\n",
       "      <td>1.097089</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>DecisionTreeModel classifier of depth 10 with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entropy</th>\n",
       "      <td>0.633398</td>\n",
       "      <td>0.749168</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>DecisionTreeModel classifier of depth 10 with ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              AUC  duration impurity  maxDepth  maxBins  \\\n",
       "gini     0.658734  1.097089     gini        10       10   \n",
       "entropy  0.633398  0.749168  entropy        10       10   \n",
       "\n",
       "                                                     model  \n",
       "gini     DecisionTreeModel classifier of depth 10 with ...  \n",
       "entropy  DecisionTreeModel classifier of depth 10 with ...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "IndexList = impurityList\n",
    "df= pd.DataFrame(metrics,index=IndexList,columns=[\"AUC\",\"duration\",\"impurity\",\"maxDepth\",\"maxBins\",\"model\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、评估maxDepth参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 评估maxDepthList参数\n",
    "impurityList = [\"gini\"]\n",
    "maxDepthList = [3,5,10,15,20,25]\n",
    "maxBinsList = [10]\n",
    "#\n",
    "# 存放结果存放至metrics中\n",
    "metrics = [trainEvaluateModel(trainData, validationData, impurity, maxDepth, maxBins)\n",
    "          for impurity in impurityList\n",
    "          for maxDepth in maxDepthList\n",
    "          for maxBins in maxBinsList]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、评估maxBins参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 评估maxBins参数\n",
    "impurityList = [\"gini\"]\n",
    "maxDepthList = [10]\n",
    "maxBinsList = [3,5,10,15,100,200]\n",
    "#\n",
    "# 存放结果存放至metrics中\n",
    "metrics = [trainEvaluateModel(trainData, validationData, impurity, maxDepth, maxBins)\n",
    "          for impurity in impurityList\n",
    "          for maxDepth in maxDepthList\n",
    "          for maxBins in maxBinsList]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、网格搜索最佳参数组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳参数组合：impurity=gini,  maxDepth=5,  maxBins=15\n",
      ",  结果AUC=0.667292662569239\n"
     ]
    }
   ],
   "source": [
    "## 定义函数gridSearch网格搜索最佳参数组合\n",
    "\n",
    "def gridSearch(trainData, validationData, impurityList, maxDepthList, maxBinsList ):\n",
    "    metrics = [trainEvaluateModel(trainData, validationData, impurity, maxDepth, maxBins)\n",
    "          for impurity in impurityList\n",
    "          for maxDepth in maxDepthList\n",
    "          for maxBins in maxBinsList]\n",
    "    # 按照AUC从大到小排序，返回最大AUC的参数组合\n",
    "    sorted_metics = sorted(metrics, key=lambda k:k[0], reverse=True)\n",
    "    best_parameters = sorted_metics[0]\n",
    "    print(\"最佳参数组合：\"+\"impurity=\"+str( best_parameters[2]) + \n",
    "         \", maxDepth=\"+str(best_parameters[3])+\", maxBins=\" + \n",
    "          str(best_parameters[4]) + \"\\n\" + \n",
    "         \",  结果AUC=\"+str(best_parameters[0]))\n",
    "    return  best_parameters\n",
    "## 参数组合\n",
    "impurityList=[\"gini\", \"entropy\"]\n",
    "maxDepthList = [3,5,10,15,20,25]\n",
    "maxBinsList = [3,5,10,15,100,200]\n",
    "\n",
    "## 调用函数返回最佳参数组合\n",
    "best_parameters = gridSearch(trainData, validationData, impurityList, maxDepthList, maxBinsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9  判断是否过拟合以及模型预测\n",
    "### 1、判断是否过拟合\n",
    "\n",
    "前面已经得到最佳参数组合impurity=entropy, maxDepth=10, maxBins=15及相应的AUC评估。使用该最佳参数组合作用于测试数据，是否会过拟合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: AUC= 0.6762603983143033\n",
      "testing: AUC= 0.6444311713864099\n"
     ]
    }
   ],
   "source": [
    "## 定义模型评估函数\n",
    "def evaluateModel(model, validationData):\n",
    "    predict = model.predict(validationData.map(lambda p:p.features))\n",
    "    predict_real = predict.zip(validationData.map(lambda p: p.label))\n",
    "    metrics = BinaryClassificationMetrics(predict_real)\n",
    "    return metrics.areaUnderROC\n",
    "\n",
    "## 使用最佳参数组合impurity=entropy,  maxDepth=10,  maxBins=15训练模型\n",
    "best_model = DecisionTree.trainClassifier(trainData, numClasses=2, categoricalFeaturesInfo={}, \n",
    "#                                          impurity=\"entropy\", maxDepth=10, maxBins=15)\n",
    "                                          impurity=\"gini\", maxDepth=5, maxBins=15)\n",
    "AUC1 = evaluateModel(best_model, trainData)\n",
    "AUC2 = evaluateModel(best_model, testData)\n",
    "print(\"training: AUC= \" + str(AUC1))\n",
    "print(\"testing: AUC= \" + str(AUC2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2、使用模型进行预测\n",
    "\n",
    "如果不考虑过拟合，依然使用上面最佳参数组合的模型对test.tsv进行预测，返回预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始导入数据...\n",
      "网址：http://www.lynnskitchenadventures.com/2009/04/homemade-enchilada-sauce.html\n",
      " ===>预测结果为: 1.0说明: 长久型(evergreen)网页\n",
      "\n",
      "网址：http://lolpics.se/18552-stun-grenade-ar\n",
      " ===>预测结果为: 0.0说明: 暂时型(ephemeral)网页\n",
      "\n",
      "网址：http://www.xcelerationfitness.com/treadmills.html\n",
      " ===>预测结果为: 0.0说明: 暂时型(ephemeral)网页\n",
      "\n",
      "网址：http://www.bloomberg.com/news/2012-02-06/syria-s-assad-deploys-tactics-of-father-to-crush-revolt-threatening-reign.html\n",
      " ===>预测结果为: 0.0说明: 暂时型(ephemeral)网页\n",
      "\n",
      "网址：http://www.wired.com/gadgetlab/2011/12/stem-turns-lemons-and-limes-into-juicy-atomizers/\n",
      " ===>预测结果为: 0.0说明: 暂时型(ephemeral)网页\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 使用模型进行预测\n",
    "def predictData(sc,model,categoriesMap):\n",
    "    print(\"开始导入数据...\")\n",
    "    rawDataWithHeader = sc.textFile(\"/usr/hduser/data/decisiontree/test.tsv\")\n",
    "    ## 取第一项数据\n",
    "    header = rawDataWithHeader.first()\n",
    "    ## 剔除字段名（特征名）行，取数据行\n",
    "    rawData = rawDataWithHeader.filter(lambda x:x!=header)\n",
    "    ## 将双引号\"替换为空字符（剔除双引号）\n",
    "    rData = rawData.map(lambda x:x.replace(\"\\\"\",\"\"))\n",
    "    ## 以制表符分割每一行\n",
    "    lines = rData.map(lambda x: x.split(\"\\t\"))\n",
    "    ## 预处理测试数据集\n",
    "    testDataRDD=lines.map(lambda r: (r[0], process_features(r, categoriesMap, len(r))))\n",
    "    DescDict={0:\"暂时型(ephemeral)网页\",\n",
    "              1:\"长久型(evergreen)网页\"}\n",
    "    ## 预测前5项数据\n",
    "    for testData in testDataRDD.take(5):\n",
    "        predictResult=model.predict(testData[1])\n",
    "        print(\"网址：\"\n",
    "              + str(testData[0])\n",
    "              + \"\\n\"\n",
    "              + \" ===>预测结果为: \"\n",
    "              + str(predictResult) + \"说明: \"\n",
    "              + DescDict[predictResult]+\"\\n\")\n",
    "\n",
    "predictData(sc,best_model,categoriesMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 查看决策树分类规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DecisionTreeModel classifier of depth 5 with 45 nodes\\n  If (feature 31 <= 1951.5)\\n   If (feature 5 <= 0.5)\\n    If (feature 31 <= 1240.5)\\n     If (feature 23 <= 0.038475772500000005)\\n      If (feature 19 <= 0.138288081)\\n       Predict: 0.0\\n      Else (feature 19 > 0.138288081)\\n       Predict: 1.0\\n     Else (feature 23 > 0.038475772500000005)\\n      If (feature 2 <= 0.5)\\n       Predict: 0.0\\n      Else (feature 2 > 0.5)\\n       Predict: 1.0\\n    Else (feature 31 > 1240.5)\\n     If (feature 3 <= 0.5)\\n      If (feature 9 <= 0.5)\\n       Predict: 1.0\\n      Else (feature 9 > 0.5)\\n       Predict: 0.0\\n     Else (feature 3 > 0.5)\\n      Predict: 0.0\\n   Else (feature 5 > 0.5)\\n    Predict: 0.0\\n  Else (feature 31 > 1951.5)\\n   If (feature 1 <= 0.5)\\n    If (feature 0 <= 0.5)\\n     If (feature 14 <= 0.4679625)\\n      If (feature 15 <= 2.4903703705)\\n       Predict: 1.0\\n      Else (feature 15 > 2.4903703705)\\n       Predict: 0.0\\n     Else (feature 14 > 0.4679625)\\n      If (feature 29 <= 20.5)\\n       Predict: 1.0\\n      Else (feature 29 > 20.5)\\n       Predict: 0.0\\n    Else (feature 0 > 0.5)\\n     If (feature 35 <= 0.119912854)\\n      Predict: 1.0\\n     Else (feature 35 > 0.119912854)\\n      If (feature 20 <= 0.5053418315)\\n       Predict: 0.0\\n      Else (feature 20 > 0.5053418315)\\n       Predict: 1.0\\n   Else (feature 1 > 0.5)\\n    If (feature 15 <= 2.4903703705)\\n     If (feature 26 <= 0.01832763)\\n      If (feature 32 <= 250.5)\\n       Predict: 1.0\\n      Else (feature 32 > 250.5)\\n       Predict: 0.0\\n     Else (feature 26 > 0.01832763)\\n      If (feature 35 <= 0.14859968699999998)\\n       Predict: 1.0\\n      Else (feature 35 > 0.14859968699999998)\\n       Predict: 0.0\\n    Else (feature 15 > 2.4903703705)\\n     If (feature 23 <= 0.0460761475)\\n      If (feature 35 <= 0.119912854)\\n       Predict: 1.0\\n      Else (feature 35 > 0.119912854)\\n       Predict: 0.0\\n     Else (feature 23 > 0.0460761475)\\n      If (feature 16 <= 0.4282032405)\\n       Predict: 0.0\\n      Else (feature 16 > 0.4282032405)\\n       Predict: 1.0\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 查看决策树分类规则\n",
    "best_model.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
